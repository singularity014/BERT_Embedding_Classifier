# Transformer_BERT_Experiment
STATUS: In progress....

In this project I've tried to apply and fine tune transformer and BERT models for some
interesting applications.

The BERT model was proposed in BERT: 
Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. 
Itâ€™s a bidirectional transformer pre-trained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.
